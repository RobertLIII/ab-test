# 📊 A/B Testing a Marketing Campaign

This project analyzes the effectiveness of a marketing campaign through A/B testing, comparing a control group to a variant group. The primary goal is to determine whether the variant leads to higher user engagement and revenue.

## 🧪 Overview

A/B testing is a widely used technique in marketing to evaluate whether a new intervention improves key outcomes. This analysis focuses on:

* **Revenue per user**: Total revenue generated by each user.
* **Conversion rate**: Whether a user made any purchase (binary).

We conduct exploratory analysis and apply statistical tests to determine if the variant offers measurable improvement over the control.

## 📁 Dataset

The dataset, sourced from Kaggle, includes:

* `USER_ID`: Unique user identifier.
* `VARIANT_NAME`: Indicates assignment to either control or variant.
* `REVENUE`: Amount of revenue generated by the user.

Some users appear multiple times, so revenue is aggregated per user to ensure one observation per person.

## 🛠️ Methodology

1. **Data Preprocessing**

   * Aggregated total revenue per user.
   * Created a binary conversion variable.
   * Separated users into control and variant groups.

2. **Exploratory Data Analysis (EDA)**

   * Visualized the revenue distribution.
   * Identified strong right skew and many zero-revenue users.

3. **Statistical Testing**

   * Welch's t-test for average revenue per user.
   * Proportion z-test for conversion rates.
   * Note: Both tests are generally robust to non-normality, especially with large sample sizes.

## 📈 Results

* **Revenue per User**

  * Welch’s t-test yielded a **t-statistic of 1.2590** and **p-value of 0.2081**.
  * No significant difference in revenue between control and variant groups.

* **Conversion Rate**

  * Z-test resulted in a **z-statistic of 0.7443** and **p-value of 0.4567**.
  * No significant difference in conversion rate between groups.

## 📉 Conclusion

This A/B test found **no statistically significant effect** of the variant on either revenue per user or conversion rate.

Despite using robust tests, the **severe right skew and high proportion of zero-revenue users** may have reduced the tests’ sensitivity. The absence of a significant result doesn’t prove the variant has no effect — it simply suggests no clear evidence of improvement was found with the current data and methods.

### 🔄 Future Considerations

* Consider **data transformations** (e.g., log or rank-based methods) or **non-parametric approaches** to better handle extreme skew.
* Explore **segmented testing** (e.g., by user type or channel) if additional features become available.
* Run **power analysis** before future tests to determine the required sample size based on expected effect size.
* Test a more impactful or targeted variant, especially if this change was minor.

## 🧾 References

* [Original write-up and insights](https://intelligenciaexmachina.com/ab-testing-marketing-campaign/)
